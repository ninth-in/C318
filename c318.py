# -*- coding: utf-8 -*-
"""c318.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eDaQRGzIwI7U5atEKoRQvyWvY4wacdfS
"""

#%% Bibliotecas utilizadas no projeto

import os
import tarfile
import urllib
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from   zlib import crc32

#%% Importação da base de dados

# Importando dados (arquivo .csv) a partir da URL (fonte)
url = 'https://raw.githubusercontent.com/ninth-in/C318/main/water_potability.csv'
df  = pd.read_csv(url)

"""# Informações sobre o dataset

1. PH

2. Hardness

3. Solids

4. Chloramines

5. Sulfate

6. Conductivity

7. Organic_carbon

8. Trihalomethanes

9. Turbidity

10. Potability

#Calculo de correlação de pares de colunas
"""

df.corr()

"""#Plotando graficos com os dados do dataset"""

import matplotlib.pyplot as plt
df.hist(figsize=(15,15))
plt.show()

#df.hist(figsize=(10,15))
sns.heatmap(df.corr(),annot=True,cmap='summer')

"""#Manipulação dos dados
Balancear o conjunto de dados desbalanceado

"""

df['ph'].fillna(value=df['ph'].median(),inplace=True)
df['Sulfate'].fillna(value=df['Sulfate'].median(),inplace=True)
df['Trihalomethanes'].fillna(value=df['Trihalomethanes'].median(),inplace=True)
X = df.drop(['Potability'], axis='columns')
y = df.Potability

#Balanceamento do dataset

from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE
from imblearn.combine import SMOTETomek, SMOTEENN


# SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_smote, y_smote = smote.fit_resample(X, y)

# ADASYN
adasyn = ADASYN(sampling_strategy='auto', random_state=42)
X_adasyn, y_adasyn = adasyn.fit_resample(X, y)

# Borderline-SMOTE
borderline_smote = BorderlineSMOTE(sampling_strategy='auto', random_state=42)
X_borderline, y_borderline = borderline_smote.fit_resample(X, y)

# SVMSMOTE
svm_smote = SVMSMOTE(sampling_strategy='auto', random_state=42)
X_svm_smote, y_svm_smote = svm_smote.fit_resample(X, y)

# SMOTE-Tomek
smote_tomek = SMOTETomek(sampling_strategy='auto', random_state=42)
X_smote_tomek, y_smote_tomek = smote_tomek.fit_resample(X, y)

# SMOTE-ENN
smote_enn = SMOTEENN(sampling_strategy='auto', random_state=42)
X_smote_enn, y_smote_enn = smote_enn.fit_resample(X, y)

from sklearn.preprocessing import MinMaxScaler


features_scaler = MinMaxScaler()


features_smote = features_scaler.fit_transform(X_smote)
features_adasyn = features_scaler.fit_transform(X_adasyn)
features_borderline = features_scaler.fit_transform(X_borderline)
features_svm_smote = features_scaler.fit_transform(X_svm_smote)
features_smote_tomek= features_scaler.fit_transform(X_smote_tomek)
features_smote_enn= features_scaler.fit_transform(X_smote_enn)

features_simple= features_scaler.fit_transform(X)

from yellowbrick.classifier import ROCAUC
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.neighbors import KNeighborsClassifier

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from lightgbm import LGBMClassifier

from sklearn.model_selection import cross_val_score

mod = []
cv_score=[]
model =[AdaBoostClassifier(), BaggingClassifier(), GradientBoostingClassifier(), DecisionTreeClassifier(),
        ExtraTreeClassifier(), KNeighborsClassifier(), SVC(), RandomForestClassifier(), LogisticRegression(),
        MLPClassifier(), LGBMClassifier() ]
for m in model:
    cv_score.append(cross_val_score(m, X_smote_enn, y_smote_enn , scoring='accuracy', cv=5).mean())
    mod.append(m)
model_df=pd.DataFrame(columns=['model','cv_score'])
model_df['model']=mod
model_df['cv_score']=cv_score
model_df.sort_values(by=['cv_score'], ascending=True).style.background_gradient(subset=['cv_score'])

from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

model_params = {
    'svm': {
        'model': SVC(gamma='auto'),
        'params' : {
            'C': [1,10,20,30,50],
            'kernel': ['rbf','linear','poly']
        }
    },
    'random_forest': {
        'model': RandomForestClassifier(),
        'params' : {
            'n_estimators': [60,70,80,100,200,300,400,500,600,700]
        }
    },
    'logistic_regression' : {
        'model': LogisticRegression(solver='liblinear',multi_class='auto'),
        'params': {
            'C': [1,5,10,15,20,25,30]
        }
    },
    'KNN' : {
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': [1,3,5,7,10,12,16,17,20,21,25,30,35,40,42,44,48,50,55,59,60]
        }
    },
    'Gradient_Boosting':
        {
            'model': GradientBoostingClassifier(),
            'params' : {
            'n_estimators': [60,70,80,100,200,300,400,500,600,700]
        }
        },
    'Bagging_Classifier':
    {
        'model': BaggingClassifier(),
        'params' : {
         'n_estimators': [60,70,80,100,200,300,400,500,600,700]
        }
    },
    'DecisionTree_Classifier':
    {
        'model': DecisionTreeClassifier(),
        'params' : { 'criterion': ['gini']
        }
    }

}

from sklearn.model_selection import GridSearchCV
scores = []

for model_name, mp in model_params.items():
    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)
    clf.fit(features_smote_enn, y_smote_enn)
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })

df_score = pd.DataFrame(scores,columns=['model','best_score','best_params'])
df_score.sort_values(by=['best_score'], ascending=True).style.background_gradient(subset=['best_score'])

"""#CrossValidation

###Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression(max_iter=500,random_state=0)
lr.fit(X_train,y_train)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
lr.coef_

lr.intercept_

y_pred_lr = lr.predict(X_test)
result = pd.DataFrame({'Actual':y_test,'predicted':y_pred_lr})
print(result)

lr.score(X_test,y_test)

"""###Support Vector Machine"""

from sklearn.svm import SVC
svm = SVC(kernel ='poly', degree = 8)
svm.fit(X_train,y_train)

y_pred_svm = lr.predict(X_test)
result = pd.DataFrame({'Actual':y_test,'predicted':y_pred_svm})
print(result)

svm.score(X_test,y_test)

"""### Decision Three Classifier

"""

model = KNeighborsClassifier(n_neighbors=9, leaf_size=20)
model.fit(X_train,y_train)
pred = model.predict(X_test)
print(classification_report(y_test, pred))
dt=accuracy_score(y_test,pred)

"""###KNN

"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=8, leaf_size=20)
knn.fit(X_train,y_train)

y_pred_knn = rfc.predict(X_test)
result = pd.DataFrame({'Actual':y_test,'predicted':y_pred_svm})
print(result)

knn.score(X_test,y_pred_knn)

"""###BaggingClassifier"""

model = BaggingClassifier(n_estimators=900)
model.fit(X_train,y_train)
pred = model.predict(X_test)
print(classification_report(y_test, pred))
bc=accuracy_score(y_test,pred)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(criterion='gini',n_estimators=50)
rfc.fit(X_train,y_train)

y_pred_rfc = rfc.predict(X_test)
result = pd.DataFrame({'Actual':y_test,'predicted':y_pred_svm})
print(result)

rfc.score(X_test,y_test)

"""###GradientBoostingClassifier"""

from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(n_estimators=700)

gb.fit(X_train,y_train)

y_pred_gb = gb.predict(X_test)

result = pd.DataFrame({'Actual':y_test,'predicted':y_pred_svm})
print(result)

gb.score(X_test,y_pred_knn)